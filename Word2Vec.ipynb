{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ofnz-2VaArBG",
        "outputId": "e3f85747-101a-455b-e6c3-809e667d0106"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import gensim\n",
        "import string\n",
        "import random\n",
        "import tarfile\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics import f1_score\n",
        "from keras.initializers import Constant\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICK3X41hEyDp"
      },
      "source": [
        "path = \"/content/drive/MyDrive/Files/aclImdb_v1.tar.gz\"\n",
        "my_tar = tarfile.open(path)\n",
        "my_tar.extractall('./')\n",
        "my_tar.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNspbAM9HQSf",
        "outputId": "13b1e6b5-9473-43d4-e73e-5bf22c3408f6"
      },
      "source": [
        "pos_dst = '/content/aclImdb/train/pos'\n",
        "neg_dst = '/content/aclImdb/train/neg'\n",
        "\n",
        "# No of positive samples\n",
        "no_ps = len(os.listdir(pos_dst))\n",
        "\n",
        "# No of negative samples\n",
        "no_ns = len(os.listdir(neg_dst))\n",
        "\n",
        "print(\"No of positive samples:\", no_ps)\n",
        "print(\"No of negative samples:\", no_ns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of positive samples: 12500\n",
            "No of negative samples: 12500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ37WfwaHviQ"
      },
      "source": [
        "train_dict = {\"pos\":[],\"neg\":[]}\n",
        "\n",
        "list_pos = os.listdir(pos_dst)\n",
        "list_neg = os.listdir(neg_dst)\n",
        "\n",
        "for pos,neg in zip(list_pos,list_neg):\n",
        "    \n",
        "    pos_f = open(os.path.join(pos_dst,pos),\"r\")\n",
        "    neg_f = open(os.path.join(neg_dst,neg),\"r\")\n",
        "    \n",
        "    # Read the text\n",
        "    pos_txt = pos_f.read()\n",
        "    neg_txt = neg_f.read()\n",
        "    \n",
        "    pos_f.close()\n",
        "    neg_f.close()\n",
        "    \n",
        "    train_dict[\"pos\"].append(pos_txt)\n",
        "    train_dict[\"neg\"].append(neg_txt) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yic7ZaPIfaJn"
      },
      "source": [
        "pos_dst_test = \"/content/aclImdb/test/pos\"\n",
        "neg_dst_test = \"/content/aclImdb/test/neg\"\n",
        "\n",
        "list_pos = os.listdir(pos_dst_test)\n",
        "list_neg = os.listdir(neg_dst_test)\n",
        "\n",
        "for pos,neg in zip(list_pos,list_neg):\n",
        "    \n",
        "    pos_f = open(os.path.join(pos_dst_test,pos),\"r\")\n",
        "    neg_f = open(os.path.join(neg_dst_test,neg),\"r\")\n",
        "    \n",
        "    # Read the text\n",
        "    pos_txt = pos_f.read()\n",
        "    neg_txt = neg_f.read()\n",
        "    \n",
        "    pos_f.close()\n",
        "    neg_f.close()\n",
        "    \n",
        "    train_dict[\"pos\"].append(pos_txt)\n",
        "    train_dict[\"neg\"].append(neg_txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkUAOM-VIIy5"
      },
      "source": [
        "X = train_dict[\"pos\"]+train_dict[\"neg\"]\n",
        "Y = np.append(np.ones((len(train_dict[\"pos\"]),1)),\n",
        "                       np.zeros((len(train_dict[\"neg\"]), 1)),\n",
        "                       axis=0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdplSFsPQpa5"
      },
      "source": [
        "def process(txt):\n",
        "    \n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    \n",
        "    # Removing hyperlinks\n",
        "    txt = re.sub(r'http\\S+', '', txt)\n",
        "    \n",
        "    # Removing Line breaks\n",
        "    txt = re.sub(r'<br />', ' ', txt)\n",
        "    \n",
        "    # Only removing the hash # sign from the word\n",
        "    txt = re.sub(r'#','', txt)\n",
        "    \n",
        "    # Removing text.text types\n",
        "    match = re.search(r'[a-zA-Z]*\\.[a-zA-Z]*', txt)\n",
        "    while(match != None):\n",
        "        replace = \" \".join((txt[match.start():match.end()].split(\".\")))\n",
        "        txt = re.sub(r'[a-zA-Z]*\\.[a-zA-Z]*',replace, txt, 1)\n",
        "        match = re.search(r'[a-zA-Z]*\\.[a-zA-Z]*', txt)\n",
        "    \n",
        "    # Removing special characters and numbers\n",
        "    pattern = r'[^a-zA-z\\s]'\n",
        "    txt = re.sub(pattern, ' ', txt)\n",
        "    \n",
        "    txt_tokens = nltk.word_tokenize(txt)  \n",
        "    \n",
        "    clean_txt = []\n",
        "    \n",
        "    for word in txt_tokens:\n",
        "        word = word.lower()\n",
        "        word = word.strip(\" \")\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            \n",
        "            stem_word = stemmer.stem(word.strip('_'))# stemming word\n",
        "            clean_txt.append(stem_word)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return clean_txt\n",
        "\n",
        "def Process_List(List):\n",
        "  P_list = []\n",
        "  for item in List:\n",
        "    P_list.append(process(item))\n",
        "  return P_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouc_Dvea--qX"
      },
      "source": [
        "X_processed = Process_List(X)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sr7GT7uMKRm",
        "outputId": "f9fe0dd1-3c20-496f-cd96-4254cfc444b2"
      },
      "source": [
        "max_length = max(len(s) for s in X_processed)\n",
        "max_length"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1422"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbI59YfrCVvx"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "model = gensim.models.Word2Vec(sentences=X_processed, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
        "\n",
        "# vocab size\n",
        "words = list(model.wv.vocab)\n",
        "print( \"Vocabulary size: %d\" % len(words))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUj3e2WtFqGg",
        "outputId": "ab07fd75-9104-4d07-92d1-2b42de6729c4"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "model.wv.most_similar(stemmer.stem(\"horrible\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terribl', 0.9086008667945862),\n",
              " ('atroci', 0.798699140548706),\n",
              " ('horrend', 0.7883786559104919),\n",
              " ('aw', 0.7870608568191528),\n",
              " ('horrid', 0.7560380697250366),\n",
              " ('pathet', 0.7318055629730225),\n",
              " ('lousi', 0.7211521863937378),\n",
              " ('dread', 0.7113169431686401),\n",
              " ('bad', 0.6869966983795166),\n",
              " ('laughabl', 0.6846331357955933)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-RZtxkCKL1N",
        "outputId": "a86277bd-0eff-48a8-a469-ca2f622225ab"
      },
      "source": [
        "model.wv.most_similar_cosmul(positive=['woman','king'],negative=['man'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('princess', 0.8513698577880859),\n",
              " ('queen', 0.8470419049263),\n",
              " ('agatha', 0.8260659575462341),\n",
              " ('chatterley', 0.8145774006843567),\n",
              " ('princ', 0.8107573390007019),\n",
              " ('carmen', 0.8095245957374573),\n",
              " ('elleri', 0.8093214631080627),\n",
              " ('changxin', 0.8087121844291687),\n",
              " ('godmoth', 0.8040111064910889),\n",
              " ('jerol', 0.8036038279533386)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V8_hOrvHEIN"
      },
      "source": [
        "filename = '/content/drive/MyDrive/Files/IMDB_50k_Embedding_Word2Vec.txt'\n",
        "model.wv.save_word2vec_format(filename,binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYbuWzHMH10K"
      },
      "source": [
        "embeddings_index = {} # Word:Embedding Dictionary\n",
        "\n",
        "f = open(os.path.join('', filename), encoding = \"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:])\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjXgGvsVI866",
        "outputId": "f5f858b2-3ad8-4ff4-ed7a-d518b48590a8"
      },
      "source": [
        "# vectorize the text samples into a 2D integer tensor\n",
        "tokenizer_obj = Tokenizer()\n",
        "tokenizer_obj.fit_on_texts(X_processed)\n",
        "sequences = tokenizer_obj.texts_to_sequences(X_processed)\n",
        "\n",
        "# pad sequences\n",
        "word_index = tokenizer_obj.word_index\n",
        "print(\"Found %s unique tokens.\" % len(word_index))\n",
        "\n",
        "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
        "print('Shape of review tensor:', review_pad.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 50106 unique tokens.\n",
            "Shape of review tensor: (25000, 1422)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kkwpZX2Jkpa"
      },
      "source": [
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM) )\n",
        "for word, i in word_index.items():\n",
        "  if i > num_words:\n",
        "    continue\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # words not found in embedding index will be all-zeros.\n",
        "    embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDzt6511Oaau"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM, \n",
        "                            embeddings_initializer=Constant(embedding_matrix), \n",
        "                            input_length=max_length,\n",
        "                            trainable=False)\n",
        "\n",
        "model.add(embedding_layer)\n",
        "model.add(GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model .compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-oBTPGF-Cld",
        "outputId": "da916ce1-fe0e-40b6-ad4c-30402435ed75"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(review_pad, Y, test_size=0.25, shuffle=True,random_state=0)\n",
        "\n",
        "y_train = y_train.ravel()\n",
        "y_test = y_test.ravel()\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train\", y_train.shape)\n",
        "print(\"Shape of X_test\", X_test.shape)\n",
        "print(\"Shape of y_test\", y_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train: (18750, 1422)\n",
            "Shape of y_train (18750,)\n",
            "Shape of X_test (6250, 1422)\n",
            "Shape of y_test (6250,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1lhf4ObbQwb"
      },
      "source": [
        "model.fit(X_train,y_train,batch_size=64,epochs=100,validation_data=(X_test,y_test),verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72YHimC1FBZ4"
      },
      "source": [
        "model.save('/content/drive/MyDrive/Files/W2V_Model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GgbSEkUaKuU",
        "outputId": "c4ca79f8-7304-47e2-a7d6-327c40746c4f"
      },
      "source": [
        "from keras.models import load_model\n",
        "m = load_model('/content/drive/MyDrive/Files/W2V_Model.h5')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek63-wGQTllK",
        "outputId": "f28573b1-f397-4deb-f927-f327e43e2146"
      },
      "source": [
        "print(\"Training Accuracy: {}%\".format(m.evaluate(X_train,y_train,verbose=0)[1]*100))\n",
        "print(\"Testing Accuracy: {}%\".format(m.evaluate(X_test,y_test,verbose=0)[1]*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 85.31200289726257%\n",
            "Testing Accuracy: 85.43999791145325%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohY1IZGpX8_o",
        "outputId": "5c202636-e447-4d07-e51d-3e7e271352c3"
      },
      "source": [
        "print(\"Training F1 Score: \",f1_score(y_train,m.predict_classes(X_train),average='binary'))\n",
        "print(\"Testing F1 Score: \",f1_score(y_test,m.predict_classes(X_test),average='binary'))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training F1 Score:  0.8509578958761772\n",
            "Testing F1 Score:  0.8529411764705883\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}